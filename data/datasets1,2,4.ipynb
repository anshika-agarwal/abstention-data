{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2x1PMQJQrmDZ",
        "outputId": "65f19800-e207-4402-972e-3a378ade5945"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (1.3.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (26.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.2.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (0.28.1)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.5.4)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (0.21.1)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2026.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.24.0->datasets) (4.12.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.24.0->datasets) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub>=0.24.0->datasets) (0.16.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->huggingface-hub>=0.24.0->datasets) (8.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMpzlpSdFfOp",
        "outputId": "894febdc-fcba-4d9f-c8fa-f434603547da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SQuAD2 train example keys: ['id', 'title', 'context', 'question', 'answers']\n",
            "SQuAD2 train example has is_impossible: False\n",
            "SQuAD2 train example answers[text] len: 1\n",
            "SQuAD 1.1 train answerables: 87599\n",
            "SQuAD 1.1 validation answerables: 10570\n",
            "SQuAD 2.0 train unanswerables: 43498\n",
            "SQuAD 2.0 validation unanswerables: 5945\n",
            "Wrote Dataset 1: 10000 -> dataset1.jsonl\n",
            "Wrote Dataset 2: 10000 -> dataset2.jsonl (answerable=5000, unanswerable=5000)\n",
            "Wrote Dataset 4: 2000 -> dataset4.jsonl (answerable=1000, unanswerable=1000)\n"
          ]
        }
      ],
      "source": [
        "# Full script (HF datasets) with robust SQuAD2.0 unanswerable detection\n",
        "# Outputs JSONL lines like: {\"id\": ..., \"input\": ..., \"output\": ..., \"split\": \"train\"|\"dev\"}\n",
        "\n",
        "import json, random\n",
        "from datasets import load_dataset\n",
        "\n",
        "NO_ANS = \"<NO-ANSWER>\"\n",
        "SEED = 42\n",
        "\n",
        "DATASET1_SIZE = 10000\n",
        "DATASET2_ANS_SIZE = 5000\n",
        "DATASET2_NOANS_SIZE = 5000\n",
        "DATASET4_ANS_SIZE = 1000\n",
        "DATASET4_NOANS_SIZE = 1000\n",
        "\n",
        "OUT_DATASET1 = \"dataset1.jsonl\"\n",
        "OUT_DATASET2 = \"dataset2.jsonl\"\n",
        "OUT_DATASET4 = \"dataset4.jsonl\"\n",
        "\n",
        "\n",
        "def write_jsonl(path, rows, split):\n",
        "    with open(path, \"w\") as f:\n",
        "        for r in rows:\n",
        "            f.write(json.dumps({\n",
        "                \"id\": r[\"id\"],\n",
        "                \"input\": r[\"question\"],\n",
        "                \"output\": r[\"label\"],\n",
        "                \"split\": split\n",
        "            }) + \"\\n\")\n",
        "\n",
        "\n",
        "def squad_answerable_rows(hf_ds):\n",
        "    \"\"\"\n",
        "    rajpurkar/squad (SQuAD 1.1): examples are answerable.\n",
        "    We still guard on having a non-empty answer.\n",
        "    \"\"\"\n",
        "    rows = []\n",
        "    for ex in hf_ds:\n",
        "        answers = ex.get(\"answers\", {})\n",
        "        texts = answers.get(\"text\", []) if isinstance(answers, dict) else []\n",
        "        if not texts:\n",
        "            continue\n",
        "        ans_text = (texts[0] or \"\").strip()\n",
        "        if not ans_text:\n",
        "            continue\n",
        "        rows.append({\n",
        "            \"id\": ex[\"id\"],\n",
        "            \"question\": ex[\"question\"].strip(),\n",
        "            \"label\": ans_text\n",
        "        })\n",
        "    return rows\n",
        "\n",
        "\n",
        "def squad_v2_unanswerable_rows(hf_ds):\n",
        "    \"\"\"\n",
        "    rajpurkar/squad_v2 (SQuAD 2.0): robustly detect unanswerables.\n",
        "    Some processed versions rely on:\n",
        "      - ex[\"is_impossible\"] == True/1\n",
        "      - OR answers[\"text\"] is empty\n",
        "    We accept either signal.\n",
        "    \"\"\"\n",
        "    rows = []\n",
        "    for ex in hf_ds:\n",
        "        answers = ex.get(\"answers\", {})\n",
        "        texts = answers.get(\"text\", []) if isinstance(answers, dict) else []\n",
        "\n",
        "        impossible_flag = bool(ex.get(\"is_impossible\", False))  # handles True/False or 1/0\n",
        "        empty_answers = (len(texts) == 0)\n",
        "\n",
        "        if impossible_flag or empty_answers:\n",
        "            rows.append({\n",
        "                \"id\": ex[\"id\"],\n",
        "                \"question\": ex[\"question\"].strip(),\n",
        "                \"label\": NO_ANS\n",
        "            })\n",
        "    return rows\n",
        "\n",
        "\n",
        "def main():\n",
        "    rng = random.Random(SEED)\n",
        "\n",
        "    # -------- Load from Hugging Face (train + validation) --------\n",
        "    squad = load_dataset(\"rajpurkar/squad\")        # train, validation\n",
        "    squad_v2 = load_dataset(\"rajpurkar/squad_v2\")  # train, validation\n",
        "\n",
        "    # (Optional) quick sanity check of SQuAD2 schema/signals\n",
        "    ex0 = squad_v2[\"train\"][0]\n",
        "    keys0 = list(ex0.keys())\n",
        "    ans0 = ex0.get(\"answers\", {})\n",
        "    ans0_texts = ans0.get(\"text\", []) if isinstance(ans0, dict) else []\n",
        "    print(\"SQuAD2 train example keys:\", keys0)\n",
        "    print(\"SQuAD2 train example has is_impossible:\", \"is_impossible\" in ex0)\n",
        "    print(\"SQuAD2 train example answers[text] len:\", len(ans0_texts))\n",
        "\n",
        "    squad_train_ans = squad_answerable_rows(squad[\"train\"])\n",
        "    squad_dev_ans   = squad_answerable_rows(squad[\"validation\"])\n",
        "\n",
        "    squadv2_train_unans = squad_v2_unanswerable_rows(squad_v2[\"train\"])\n",
        "    squadv2_dev_unans   = squad_v2_unanswerable_rows(squad_v2[\"validation\"])\n",
        "\n",
        "    print(\"SQuAD 1.1 train answerables:\", len(squad_train_ans))\n",
        "    print(\"SQuAD 1.1 validation answerables:\", len(squad_dev_ans))\n",
        "    print(\"SQuAD 2.0 train unanswerables:\", len(squadv2_train_unans))\n",
        "    print(\"SQuAD 2.0 validation unanswerables:\", len(squadv2_dev_unans))\n",
        "\n",
        "    # -------- Dataset 1 (train): 10k answerables from SQuAD 1.1 train --------\n",
        "    if len(squad_train_ans) < DATASET1_SIZE:\n",
        "        raise ValueError(\n",
        "            f\"Not enough SQuAD 1.1 train answerables for Dataset 1. \"\n",
        "            f\"Have {len(squad_train_ans)}, need {DATASET1_SIZE}.\"\n",
        "        )\n",
        "    dataset1 = rng.sample(squad_train_ans, DATASET1_SIZE)\n",
        "    write_jsonl(OUT_DATASET1, dataset1, split=\"train\")\n",
        "    print(f\"Wrote Dataset 1: {len(dataset1)} -> {OUT_DATASET1}\")\n",
        "\n",
        "    # -------- Dataset 2 (train): 5k from dataset1 + 5k SQuAD 2.0 train unanswerables --------\n",
        "    if len(dataset1) < DATASET2_ANS_SIZE:\n",
        "        raise ValueError(\"Dataset 1 too small to sample answerables for Dataset 2.\")\n",
        "    if len(squadv2_train_unans) < DATASET2_NOANS_SIZE:\n",
        "        raise ValueError(\n",
        "            f\"Not enough SQuAD 2.0 train unanswerables for Dataset 2. \"\n",
        "            f\"Have {len(squadv2_train_unans)}, need {DATASET2_NOANS_SIZE}.\\n\"\n",
        "            f\"If this is unexpected, check the printed schema signals above.\"\n",
        "        )\n",
        "\n",
        "    dataset2_ans = rng.sample(dataset1, DATASET2_ANS_SIZE)\n",
        "    dataset2_unans = rng.sample(squadv2_train_unans, DATASET2_NOANS_SIZE)\n",
        "\n",
        "    dataset2 = dataset2_ans + dataset2_unans\n",
        "    rng.shuffle(dataset2)\n",
        "    write_jsonl(OUT_DATASET2, dataset2, split=\"train\")\n",
        "    print(f\"Wrote Dataset 2: {len(dataset2)} -> {OUT_DATASET2} \"\n",
        "          f\"(answerable={len(dataset2_ans)}, unanswerable={len(dataset2_unans)})\")\n",
        "\n",
        "    # -------- Dataset 4 (dev held-out): from validation splits only --------\n",
        "    if len(squad_dev_ans) < DATASET4_ANS_SIZE:\n",
        "        raise ValueError(\n",
        "            f\"Not enough SQuAD 1.1 validation answerables for Dataset 4. \"\n",
        "            f\"Have {len(squad_dev_ans)}, need {DATASET4_ANS_SIZE}.\"\n",
        "        )\n",
        "    if len(squadv2_dev_unans) < DATASET4_NOANS_SIZE:\n",
        "        raise ValueError(\n",
        "            f\"Not enough SQuAD 2.0 validation unanswerables for Dataset 4. \"\n",
        "            f\"Have {len(squadv2_dev_unans)}, need {DATASET4_NOANS_SIZE}.\"\n",
        "        )\n",
        "\n",
        "    dataset4_ans = rng.sample(squad_dev_ans, DATASET4_ANS_SIZE)\n",
        "    dataset4_unans = rng.sample(squadv2_dev_unans, DATASET4_NOANS_SIZE)\n",
        "\n",
        "    dataset4 = dataset4_ans + dataset4_unans\n",
        "    rng.shuffle(dataset4)\n",
        "    write_jsonl(OUT_DATASET4, dataset4, split=\"dev\")\n",
        "    print(f\"Wrote Dataset 4: {len(dataset4)} -> {OUT_DATASET4} \"\n",
        "          f\"(answerable={len(dataset4_ans)}, unanswerable={len(dataset4_unans)})\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}